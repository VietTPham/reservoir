/* HJSON configuration file for use with MacLean Lab SNN experiments
 *
 * The HJSON syntax is fully defined here: https://hjson.github.io/
 * ---------------------------------------------------------------------------
 * This file is configured for a model trained to match a lightly-noised
 * sinusoid pattern. Its primary purpose is to serve as an example and/or
 * template for how the repository structure works.
 *
 * Some small notes about encoding Python variables in HJSON:
 * - HJSON booleans are lowercase (`true` & `false`, not `True` & `False`)
 * - Use `null` in HJSON where you want `None` in Python
 */
{
    #┬───────────────────────────────────────────────────────────────────────╮
    #┤ File Saving                                                           │
    #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
    #┤ Configure what gets saved, where, and how. Please be mindful of other │
    #┤ researchers and their data when adjusting these settings.             │
    #┤                                                                       │
    #┤ Filepaths are relative to where the script was invoked (not           │
    #┤ necessarily) where the script is located. See the documentation below │
    #┤ for how directories relate to each other and what each should         │
    #┤ contain.                                                              │
    #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
    #┤******************* THESE SETTINGS CAN DELETE FILES *******************│
    #┴───────────────────────────────────────────────────────────────────────╯
    save:
    {
        #┬───────────────────────────────────────────────────────────────────╮
        #┤ Directories                                                       │
        #┴───────────────────────────────────────────────────────────────────╯

        // Directory containing this experiment's output. Prefixes all other
        // directory paths.
        //
        // Full path: {exp_dir}
        //
        // Mandatory variable.
        exp_dir: /data/experiments/run-batch30-dualloss-specinput0.3

        // Collection of directories which will be placed inside exp_dir
        //
        // Mandatory namespace.
        subdirs:
        {
            #┬───────────────────────────────────────────────────────────────╮
            #┤ TensorFlow Directories                                        │
            #┴───────────────────────────────────────────────────────────────╯

            // Directory storing checkpoints intended to restore models for
            // training (`.ckpt` or `.h5` files, generally) or holding weights
            //
            // Full path: {exp_dir}\{checkpoint_dir}
            checkpoint_dir: checkpoints

            #┬───────────────────────────────────────────────────────────────╮
            #┤ TensorBoard Directories                                       │
            #┴───────────────────────────────────────────────────────────────╯

            // Directory storing summary data from one or more training
            // sessions
            summary_dir: tensorboard/summaries

            // Directory storing logdirs (*only* for use with TensorBoard)
            profile_dir: tensorboard/profile

            #┬───────────────────────────────────────────────────────────────╮
            #┤ Other Directories                                             │
            #┴───────────────────────────────────────────────────────────────╯

            // Used to store plots
            plot_dir: plots

            // Big everything jumbo place of stuff (the important stuff)
            main_output_dir: npz-data
        }

        #┬───────────────────────────────────────────────────────────────────╮
        #┤ Flags (Mandatory)                                                 │
        #┴───────────────────────────────────────────────────────────────────╯

        // When enabled, the exp_dir will have a number suffixed to avoid
        // overwriting saved data
        avoid_overwrite: true

        // If avoid_overwrite is set to false, and exp_dir overwrites a
        // directory, this will purge the contents of the old directory. This
        // is to avoid mixing data from different experiments, but be careful.
        hard_overwrite: false

        // Experiment directories will be suffixed by a UNIX timestamp
        timestamp: true

        // Save a copy of this file to exp_dir, so there's a record of what
        // parameters the experiment was run with (won't save any indication
        // of what the `.py` files were like)
        log_config: true

        // Enable/disable any additional processing on the data performed
        // after training (doesn't do anything right now [?])
        postprocess: true

        #┬───────────────────────────────────────────────────────────────────╮
        #┤ Flags (Supplemental)                                              │
        #┴───────────────────────────────────────────────────────────────────╯

        /* if there are any flags for saving particular to your experiment,
         * include them here */
        save_npz: true
        save_loss_only: false
    }

    #┬───────────────────────────────────────────────────────────────────────╮
    #┤ Model-Specific Parameters                                             │
    #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
    #┤ Any parameter used to define your experiment's models and layers.     │
    #┴───────────────────────────────────────────────────────────────────────╯
    model:
    {
        type: std_backprop

        cell:
        {
            type: ExInALIF

            freewiring: false   // now the term for our original unprincipled use of "rewiring"

            rewiring: true   // now the term for permitting a new connection to grow if/when an existing connection disappears

            output_rewiring: true

            input_rewiring: false

            units: 300   // number of units in the layer

            thr: -50.4   // threshold [mV]
            EL: -70.6    // [mV]
            n_refrac: 4  // number of refractory ms
            tau: 20.0    //

            refrac_stop_grad: true

            dampening_factor: 0.3  // for LIF variants
            // dampening_factor: 0.2  // for AdEx variants

            frac_e: 0.8

            specify_input: true
            soft_specify_input: false
            specify_lognormal_input: false

            // These parameters are used for input-to-main layer if specify_input is true
            p_input: 0.3 // this is the one we have been using, which is to excitatory units only
            // we would use the following if we have input to both e and i units
            p_to_e: 0.2
            p_to_i: 0.1

            // Multiplier for input weights (to increase or decrease strengths generated from IMG)
            input_multiplier: 15

            // These parameters will only be used for ExIn neurons
            p_ee: 0.160
            p_ei: 0.205
            p_ie: 0.252
            p_ii: 0.284

            // Output layer is fully dense or sparse
            // If sparse, uses the p_ee and p_ie values above from the e and i RSNN units
            define_output_w: true
            p_eo: 0.160
            p_io: 0.252

            output_multiplier: 1

            // ...

            beta: 0.16
            tau_adaptation: 100

            // μ for normal distribution (exponentiated for lognormal weights)
            mu: -0.64

            // σ for normal distribution (exponentiated for lognormal weights)
            sigma: 0.51

            // A bunch of AdEx parameters all at once with no context
            tauw: 144
            a: 0.000004
            b: 0.0000805
            gL: 0.00003
            C: 281
            deltaT: 2
            V_reset: -51

            // output schemes
            categorical_output: false
            likelihood_output: false
            swap_output_labels: false
        }
    }

    #┬───────────────────────────────────────────────────────────────────────╮
    #┤ Data Config                                                           │
    #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
    #┤ Variables parameterizing the datasets used to train the model.        │
    #┴───────────────────────────────────────────────────────────────────────╯
    data:
    {
        type: ccd

        // file containing firing rate data for CCD (to regenerate spikes)
        rate_npy:
        /data/datasets/CNN_outputs/ch8_abs_ccd_rates.npy

        // file containing coherence change data (CCD) corresponding to the rates
        coh_npz:
        /data/datasets/CNN_outputs/ch8_abs_ccd_coherences.npz

        // file containing spike data for CCD
        spike_npz:
        /data/datasets/CNN_outputs/spike_train_mixed_limlifetime_abs.npz
        //
        seq_len: 4080

        // number of input units
        n_input: 16
    }

    #┬───────────────────────────────────────────────────────────────────────╮
    #┤ Logging Config                                                        │
    #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
    #┤ Choose what to log during training and how.                           │
    #┴───────────────────────────────────────────────────────────────────────╯
    log:
    {
        // logger to use
        type: std_timeseries

        // Whitelist and blacklist for logvars
        //
        // Logvars with labels in these lists won't be collected or processed
        // at all by the logger. A value of `null` will disable the list, a
        // value of [] will create an empty but functioning white/black -list.
        //
        // Blacklist values take precedence over whitelist values.
        //
        // TODO: add list of potential logged variables
        logvar_whitelist: null
        logvar_blacklist: null

        // Whitelist and blacklist for writing to disk
        //
        // Unlike the logvar lists, these values will still be stored in the
        // logger, they just won't be written to disk. This is useful if you
        // want to generate plots but not keep the underlying data.
        todisk_whitelist: [
            step_rate_loss
            step_task_loss
            epoch_loss
            spikes
            true_y
            pred_y
            tv0.postweights
            tv0.gradients
            tv1.postweights
            tv1.gradients
            tv2.postweights
            tv2.gradients
        ]
        todisk_blacklist: null

        // Save npz files or not
        log_npz: true

        // Save logging values to disk every n epochs
        post_every: 10

        // Toggle the TensorBoard profiler
        run_profiler: false

        // If enabled, the profiler will run on each epoch in the list below
        profiler_epochs: [
            4
        ]

        // Filter for a subset of the logging procedures in the training loop
        layer_whitelist: [
            rnn
        ]

        // Floating point precision of numpy arrays saved to disk (fewer bits
        // consume less memory on disk)
        //
        // 64  -->  ~16 decimal precision
        // 32  -->  ~7 decimal precision
        // 16  -->  ~3 decimal precision
        float_dtype: float16

        // Integer size
        int_dtype: uint16

        #┬───────────────────────────────────────────────────────────────────╮
        #┤ Model Saving and Checkpoints                                      │
        #┴───────────────────────────────────────────────────────────────────╯

        // [!] still trying to figure out how to save models as SavedModel
        // [!] we can save checkpoints and read them with
        //     tf.train.load_checkpointbut I'm still trying to figure out how
        //     to integrate them into the training infrastructure

        // Create a checkpoint every N epochs (checkpoints disabled if N=0)
        ckpt_freq: 0

        // Maximum number of checkpoints saved per experiment (will save the
        // most recent)
        ckpt_lim: 0
    }

    #┬───────────────────────────────────────────────────────────────────────╮
    #┤ Training Config                                                       │
    #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
    #┤ Variables parameterizing training.                                    │
    #┴───────────────────────────────────────────────────────────────────────╯
    train:
    {
        type: std_single_task

        use_adam: true
        //alternative is SGD

        simple_rate_loss: false

        noise_weights_before_gradient: false
        noise_weights_after_gradient: false
        lax_rate_loss: false
        lax_rate_threshold: 0.25
        // if rate_loss < lax_rate_threshold*task_loss, do not include rate loss
        lax_synch_loss: false
        lax_synch_threshold: 0.25

        input_trainable: true
        //note that when false, this means tv0 is the recurrent weights instead of tv1

        output_trainable: true
        //note that when false, this means there is no tv associated with the output layer

        learning_rate: 0.001

        output_learning_rate: 0.00001

        // number of trials in batch
        batch_size: 30

        // number of batches in an epoch
        n_batch: 10

        // number of epochs the model is fit over
        n_epochs: 1000

        redraw_output: false

        silence_input_to_proj: false

        silence_input_to_nonproj: false

        silence_nonproj: false

        silence_proj: false

        matched_silencing: false

        silencing_threshold: 1

        include_task_loss: true

        include_rate_loss: true

        include_synch_loss: false

        target_rate: 0.02

        rate_cost: 0.1

        target_synch: 2.0

        synch_cost: 0.1

        target_conn: 0.266
        // mean of initial p's, which is imprecise

        conn_cost: 0.1
    }

    #┬───────────────────────────────────────────────────────────────────────╮
    #┤ Miscellaneous                                                         │
    #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
    #┤ Any other values useful to manipulate from this file.                 │
    #┴───────────────────────────────────────────────────────────────────────╯
    misc:
    {
        // time step
        dt: 1
    }
}
