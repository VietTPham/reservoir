/* HJSON configuration file for use with MacLean-1 SNN experiments
 * ---------------------------------------------------------------------------
 * This file is configured for a model trained to match a lightly-noised
 * sinusoid pattern. Its primary purpose is to serve as an example and/or
 * template for how the repository structure works.
 *
 * Some small notes about encoding Python variables in HJSON:
 * - HJSON booleans are lowercase (`true` & `false`, not `True` & `False`)
 * - Use `null` in HJSON where you want `None` in Python
 *
 * The HJSON syntax is fully defined here: https://hjson.github.io/
 */
{
  #┬───────────────────────────────────────────────────────────────────────╮
  #┤ Model-Specific Parameters                                             │
  #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
  #┤ Model-specific parameters used to construct the network used in the   │
  #┤ experiment. These values should be a one-to-one analog with the       │
  #┤ target model's attributes (i.e. variable names and relative object    │
  #┤ structures need to align between the two). It's fine to omit          │
  #┤ parameters with default values.                                       │
  #┤                                                                       │
  #┤ If you need to access data in `cfg` during model initialization, or   │
  #┤ would like to have `cfg` be an attribute in a model for any other     │
  #┤ reason, you need to indicate so in this script by adding `cfg: null`  │
  #┤ (nested in the structure needing access).                             │
  #┴───────────────────────────────────────────────────────────────────────╯
  model:
  {

    #┬───────────────────────────────────────────────────────────────────╮
    #┤ Sub-Network & Layer Parameters                                    │
    #┴───────────────────────────────────────────────────────────────────╯
    cell:
    // currently configured to support all LIF and AdEx class neurons:
    {
      #┬───────────────────────────────────────────────────────────────╮
      #┤ Flags                                                         │
      #┴───────────────────────────────────────────────────────────────╯

      cfg: null

      #┬───────────────────────────────────────────────────────────────╮
      #┤ Values                                                        │
      #┴───────────────────────────────────────────────────────────────╯

      rewiring: false

      units: 100   // number of units in the layer

      thr: -50.4   // threshold [mV]
      EL: -70.6    // [mV]
      n_refrac: 1  // number of refractory ms
      tau: 20.0    //

      dampening_factor: 0.3

      frac_e: 0.8  // fraction of total units that are excitatory

      p: {
        "ee": 0.160
        "ei": 0.244
        "ie": 0.318
        "ii": 0.343
      }

      // specific to ALIF neurons:
      beta: 0.16
      tau_adaptation: 100

      // specific to AdEx neurons:
      //tauw: 144       // [ms]
      //a: 4 / 1e6      // [mSiemens] originally 4 * nSiemens
      //b: 80.5 / 1e6   // [mV * ms] originally 80.5 * pAmpere
      //gL: 30 / 1e6    // [mSiemens] originally 30 * nSiemens
      //C: 281          // [mSiemens * ms] originally 281 * uFarad
      //deltaT: 2       // [mV]
      //V_reset: -70.6  // [mV]

      //Our base units (ones place) are mV and ms and mSiemens
      //Volt = 1e3
      //Siemens = 1e3
      //Second = 1e3
      //Ampere = Volt * Siemens
      //mAmpere = Ampere / 1e3
      //nAmpere = Ampere / 1e9
      //pAmpere = Ampere / 1e12
      //Farad = Ampere * Second / Volt
      //Ohm = 1 / Siemens
      //MOhm = Ohm * 1e6
      //uFarad = Farad / 1e6
      //mSecond = Second / 1e3
      //mVolt = Volt / 1e3
      //mSiemens = Siemens / 1e3
      //nSiemens = Siemens / 1e9
      //Hertz = 1 / Second

    }
  }

  #┬───────────────────────────────────────────────────────────────────────╮
  #┤ File Saving                                                           │
  #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
  #┤ Configure what gets saved, where, and how. Please be mindful of other │
  #┤ researchers and their data when adjusting these settings.             │
  #┤                                                                       │
  #┤ Filepaths are relative to where the script was invoked (not           │
  #┤ necessarily) where the script is located. See the documentation below │
  #┤ for how directories relate to each other and what each should         │
  #┤ contain.                                                              │
  #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
  #┤******************* THESE SETTINGS CAN DELETE FILES *******************│
  #┴───────────────────────────────────────────────────────────────────────╯
  save:
  {
    #┬───────────────────────────────────────────────────────────────────╮
    #┤ Directories                                                       │
    #┴───────────────────────────────────────────────────────────────────╯

    // Directory containing this experiment's output. Prefixes all other
    // directory paths.
    //
    // Full path: {exp_dir}
    exp_dir: ../experiments/dmc_category_lr0.01

    // Collection of directories which will be placed inside exp_dir
    subdirs:
    {
      #┬───────────────────────────────────────────────────────────────╮
      #┤ Standard                                                      │
      #┴───────────────────────────────────────────────────────────────╯

      // Directory storing checkpoints intended to restore models for
      // training (`.ckpt` or `.h5` files, generally) or holding weights
      //
      // Full path: {exp_dir}\{checkpoint_dir}
      checkpoint_dir: checkpoints

      // Directory storing summary data from one or more training
      // sessions
      //
      // Full path: {exp_dir}\{summary_dir}
      summary_dir: summaries

      // Directory storing logdirs (*only* for use with TensorBoard)
      //
      // Full path: {exp_dir}\{tb_logdir}
      tb_logdir: logdirs

      #┬───────────────────────────────────────────────────────────────╮
      #┤ Supplementary                                                 │
      #┴───────────────────────────────────────────────────────────────╯

      // Used to store plots
      plot_dir: plots

      // Big everything jumbo place of stuff (the important stuff)
      main_output_dir: npz-data

      /* you can add more directories you want to create/use here */
    }

    #┬───────────────────────────────────────────────────────────────────╮
    #┤ Flags (Mandatory)                                                 │
    #┴───────────────────────────────────────────────────────────────────╯

    // When enabled, the exp_dir will have a number suffixed to avoid
    // overwriting saved data
    avoid_overwrite: true

    // If avoid_overwrite is set to false, and exp_dir overwrites a
    // directory, this will purge the contents of the old directory. This
    // is to avoid mixing data from different experiments, but be careful.
    hard_overwrite: false

    // Experiment directories will be suffixed by a UNIX timestamp
    timestamp: false

    // Save a copy of this file to exp_dir, so there's a record of what
    // parameters the experiment was run with (won't save any indication
    // of what the `.py` files were like)
    log_config: true

    // Enable/disable any additional processing on the data performed
    // after training
    postprocess: true

    #┬───────────────────────────────────────────────────────────────────╮
    #┤ Flags (Supplemental)                                              │
    #┴───────────────────────────────────────────────────────────────────╯

    /* if there are any flags for saving particular to your experiment,
     * include them here */
    // whether to save the npz data
    save_npz: false

    // whether to save the losses only
    save_loss_only: false
  }

  #┬───────────────────────────────────────────────────────────────────────╮
  #┤ Data Config                                                           │
  #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
  #┤ Variables parameterizing the datasets used to train the model.        │
  #┴───────────────────────────────────────────────────────────────────────╯
  data:
  {
    // file containing spike data
    spike_npz:
    /home/macleanlab/stim/dmc/2021-08-05-1225/CNN_outputs/spike_train.npz

    // file containing sample match/non-match data
    match_npy:
    /home/macleanlab/stim/dmc/2021-08-05-1225/CNN_outputs/cat_time_series.npy

    //
    seq_len: 4080

    // number of input units
    n_input: 16
  }

  #┬───────────────────────────────────────────────────────────────────────╮
  #┤ Logging Config                                                        │
  #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
  #┤ Choose what to log during training and how.                           │
  #┴───────────────────────────────────────────────────────────────────────╯
  log:
  {
    // Save logging values to disk every n epochs
    post_every: 10

    // Neuron used ([!] would like to automate this):
    neuron: ExInALIF

    // Training used ([!] would like to automate this):
    training_method: backprop

    // Toggle the TensorBoard profiler
    run_profiler: false

    // If enabled, the profiler will run on each epoch in the list below
    profiler_epochs: [
      4
    ]

    // Filter for a subset of the logging procedures in the training loop
    layer_whitelist: [
      rnn
    ]

    // Floating point precision of numpy arrays saved to disk (fewer bits
    // consume less memory on disk)
    //
    // 64  -->  ~16 decimal precision
    // 32  -->  ~7 decimal precision
    // 16  -->  ~3 decimal precision
    float_dtype: float16

    // Integer size
    int_dtype: uint16
  }

  #┬───────────────────────────────────────────────────────────────────────╮
  #┤ Training Config                                                       │
  #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
  #┤ Variables parameterizing training.                                    │
  #┴───────────────────────────────────────────────────────────────────────╯
  train:
  {
    learning_rate: 0.01

    // number of trials in batch
    batch_size: 5

    // number of batches in an epoch
    n_batch: 10

    // number of epochs the model is fit over
    n_epochs: 200

    voltage_cost: 0.1

    target_synch: 2.0

    synch_cost: 0.05

    target_rate: 0.02

    rate_cost: 10.0

    target_conn: 0.266
    // mean of initial p's

    conn_cost: 0.1
  }

  #┬───────────────────────────────────────────────────────────────────────╮
  #┤ Miscellaneous                                                         │
  #┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┤
  #┤ Any other values useful to manipulate from this file.                 │
  #┴───────────────────────────────────────────────────────────────────────╯
  misc:
  {
    // time step
    dt: 1

    //
    n_recurrent: 100

    // μ for normal distribution (exponentiated for lognormal weights)
    mu: -0.64

    // σ for normal distribution (exponentiated for lognormal weights)
    sigma: 0.51
  }
}
